{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image\n",
    "# detect text boxes\n",
    "# detect text\n",
    "# return text and positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from dotmap import DotMap\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from craft import CRAFT\n",
    "import imgproc\n",
    "import craft_utils\n",
    "\n",
    "from ocr_model import Model\n",
    "from ocr_dataset import RawDataset, AlignCollate\n",
    "from ocr_utils import CTCLabelConverter, AttnLabelConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RepresentsInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "# end\n",
    "\n",
    "def copyStateDict(state_dict):\n",
    "    if list(state_dict.keys())[0].startswith(\"module\"):\n",
    "        start_idx = 1\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = \".\".join(k.split(\".\")[start_idx:])\n",
    "        new_state_dict[name] = v\n",
    "    return new_state_dict\n",
    "# end\n",
    "\n",
    "def init_craft():\n",
    "     # load net\n",
    "    net = CRAFT()     # initialize\n",
    "    str_trained_model_path = 'dmodel/craft_mlt_25k.pth'\n",
    "    net.load_state_dict(copyStateDict(torch.load(str_trained_model_path)))\n",
    "    \n",
    "    if True:\n",
    "        net = net.cuda()\n",
    "        net = torch.nn.DataParallel(net)\n",
    "        cudnn.benchmark = False\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    return net\n",
    "# end\n",
    "\n",
    "def test_net(net, image, text_threshold, link_threshold, low_text, cuda, canvas_size, show_time,mag_ratio):\n",
    "    t0 = time.time()\n",
    "\n",
    "    # resize\n",
    "    img_resized, target_ratio, size_heatmap = imgproc.resize_aspect_ratio(image, canvas_size, interpolation=cv2.INTER_LINEAR, mag_ratio=mag_ratio)\n",
    "    ratio_h = ratio_w = 1 / target_ratio\n",
    "\n",
    "    # preprocessing\n",
    "    x = imgproc.normalizeMeanVariance(img_resized)\n",
    "    x = torch.from_numpy(x).permute(2, 0, 1)    # [h, w, c] to [c, h, w]\n",
    "    x = Variable(x.unsqueeze(0))                # [c, h, w] to [b, c, h, w]\n",
    "    if cuda:\n",
    "        x = x.cuda()\n",
    "\n",
    "    # forward pass\n",
    "    y, _ = net(x)\n",
    "\n",
    "    # make score and link map\n",
    "    score_text = y[0,:,:,0].cpu().data.numpy()\n",
    "    score_link = y[0,:,:,1].cpu().data.numpy()\n",
    "\n",
    "    t0 = time.time() - t0\n",
    "    t1 = time.time()\n",
    "    \n",
    "    # Post-processing\n",
    "    boxes = craft_utils.getDetBoxes(score_text, score_link, text_threshold, link_threshold, low_text)\n",
    "    boxes = craft_utils.adjustResultCoordinates(boxes, ratio_w, ratio_h)\n",
    "\n",
    "    t1 = time.time() - t1\n",
    "    \n",
    "    if show_time : print(\"\\ninfer/postproc time : {:.3f}/{:.3f}\".format(t0, t1))\n",
    "\n",
    "    return boxes\n",
    "# end\n",
    "\n",
    "def init_ocr(opt):\n",
    "    \"\"\" model configuration \"\"\"\n",
    "    if 'CTC' in opt['Prediction']:\n",
    "        converter = CTCLabelConverter(opt['character'])\n",
    "    else:\n",
    "        converter = AttnLabelConverter(opt['character'])\n",
    "    opt['num_class'] = len(converter.character)\n",
    "\n",
    "    if opt['rgb']:\n",
    "        opt['input_channel'] = 3\n",
    "    model = Model(opt)\n",
    "    print('model input parameters', opt['imgH'], opt['imgW'], opt['num_fiducial'], opt['input_channel'], opt['output_channel'],\n",
    "          opt['hidden_size'], opt['num_class'], opt['batch_max_length'], opt['Transformation'], opt['FeatureExtraction'],\n",
    "          opt['SequenceModeling'], opt['Prediction'])\n",
    "\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    # load model\n",
    "    print('loading pretrained model from %s' % opt['saved_model'])\n",
    "    model.load_state_dict(torch.load(opt['saved_model']))\n",
    "\n",
    "    # prepare data. two demo images from https://github.com/bgshih/crnn#run-demo\n",
    "    # sai removed here\n",
    "\n",
    "    # predict\n",
    "    model.eval()\n",
    "    \n",
    "    return (model, converter)\n",
    "# end    \n",
    "\n",
    "def run_ocr(model, converter, image_tensors, opt):    \n",
    "    batch_size = image_tensors.size(0)\n",
    "    with torch.no_grad():\n",
    "        image = image_tensors.cuda()\n",
    "        # For max length prediction\n",
    "        length_for_pred = torch.cuda.IntTensor([opt['batch_max_length']] * batch_size)\n",
    "        text_for_pred = torch.cuda.LongTensor(batch_size, opt['batch_max_length'] + 1).fill_(0)\n",
    "\n",
    "    if 'CTC' in opt['Prediction']:\n",
    "        preds = model(image, text_for_pred).log_softmax(2)\n",
    "\n",
    "        # Select max probabilty (greedy decoding) then decode index to character\n",
    "        preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "        _, preds_index = preds.permute(1, 0, 2).max(2)\n",
    "        preds_index = preds_index.transpose(1, 0).contiguous().view(-1)\n",
    "        preds_str = converter.decode(preds_index.data, preds_size.data)\n",
    "\n",
    "    else:\n",
    "        preds = model(image, text_for_pred, is_train=False)\n",
    "\n",
    "        # select max probabilty (greedy decoding) then decode index to character\n",
    "        _, preds_index = preds.max(2)\n",
    "        preds_str = converter.decode(preds_index, length_for_pred)\n",
    "    # end\n",
    "\n",
    "    # print('-' * 80)\n",
    "    # print('image_path\\tpredicted_labels')\n",
    "    # print('-' * 80)\n",
    "    \n",
    "    pred_str_arr = []\n",
    "    \n",
    "    for index, pred in enumerate(preds_str):\n",
    "        if 'Attn' in opt['Prediction']:\n",
    "            pred = pred[:pred.find('[s]')]  # prune after \"end of sentence\" token ([s])\n",
    "        # end\n",
    "        pred_str_arr.append(pred)\n",
    "        # print('img_name<',index, '> pred<',pred, '>')\n",
    "    # end\n",
    "    \n",
    "    return pred_str_arr\n",
    "# end\n",
    "\n",
    "def preprocess_and_convert_to_tensorsarr(img_arr, tw, th):\n",
    "    image_tensors = []\n",
    "    for img in img_arr:\n",
    "        img = Image.fromarray(img)\n",
    "        img = img.resize((tw, th), Image.BICUBIC)\n",
    "        img = transforms.ToTensor()(img)\n",
    "        img.sub_(0.5).div_(0.5)\n",
    "        image_tensors.append(img)\n",
    "    # end\n",
    "    \n",
    "    image_tensors = torch.cat([t.unsqueeze(0) for t in image_tensors], 0)\n",
    "    \n",
    "    return image_tensors\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image, t1_name, t2_name):\n",
    "\n",
    "    bboxes = test_net(net, image, text_threshold, link_threshold, low_text, cuda, canvas_size, show_time, mag_ratio)\n",
    "\n",
    "    b_plot = False\n",
    "    len_boxes = len(bboxes)\n",
    "    crop_img_arr = []\n",
    "\n",
    "    for index, box in enumerate(bboxes):\n",
    "\n",
    "        box = box.astype(np.int32)\n",
    "        x = box[0][0]\n",
    "        w = box[1][0]-box[0][0]\n",
    "        y = box[0][1]\n",
    "        h = box[3][1]-box[0][1]\n",
    "\n",
    "        delta = 0\n",
    "        x = x - delta\n",
    "        y = y - delta\n",
    "        w = w + 2*delta\n",
    "        h = h + 2*delta\n",
    "\n",
    "        if x < 0 or y < 0 or x+w > image.shape[1] or y+h > image.shape[0]:\n",
    "            continue\n",
    "        # end                   \n",
    "\n",
    "        crop_img = image[y:y+h, x:x+w]   \n",
    "        # crop_img_arr.append(crop_img)\n",
    "        crop_img_arr.append(cv2.cvtColor(crop_img, cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "        if b_plot:    \n",
    "            plt.subplot(len_boxes, 1, index+1)\n",
    "            plt.imshow(crop_img)\n",
    "        # end\n",
    "\n",
    "    # end\n",
    "\n",
    "    if b_plot:\n",
    "        plt.show()\n",
    "    # end   \n",
    "    \n",
    "    if len_boxes == 0:\n",
    "        return {}\n",
    "    # end\n",
    "\n",
    "    image_tensors = preprocess_and_convert_to_tensorsarr(crop_img_arr, opt['imgW'], opt['imgH'])\n",
    "\n",
    "    pred_text_arr = run_ocr(ocr_model, ocr_converter, image_tensors, opt)\n",
    "    \n",
    "    if len(pred_text_arr) != len_boxes:\n",
    "        print('boxes len<', len_boxes, '> pred len<', len(pred_text_arr), '>')\n",
    "        # raise('pred and boxes dont match. Not expected to come here')\n",
    "        return {}\n",
    "    # end\n",
    "    \n",
    "    t1_name = t1_name.lower()\n",
    "    t2_name = t2_name.lower()\n",
    "    pos_out = {}\n",
    "    for index, pred_text in enumerate(pred_text_arr):\n",
    "        if pred_text == t1_name:\n",
    "            pos_out['t1'] = bboxes[index]\n",
    "        # end\n",
    "        \n",
    "        if pred_text == t2_name:\n",
    "            pos_out['t2'] = bboxes[index]\n",
    "        # end \n",
    "    # end\n",
    "    \n",
    "    return pos_out\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_scoreboard(image, bboxes):\n",
    "    \n",
    "    b_plot = False\n",
    "    len_boxes = len(bboxes)\n",
    "    crop_img_arr = []\n",
    "\n",
    "    for index, box in enumerate(bboxes):\n",
    "\n",
    "        box = box.astype(np.int32)\n",
    "        x = box[0][0]\n",
    "        w = box[1][0]-box[0][0]\n",
    "        y = box[0][1]\n",
    "        h = box[3][1]-box[0][1]\n",
    "\n",
    "        delta = 0\n",
    "        x = x - delta\n",
    "        y = y - delta\n",
    "        w = w + 2*delta\n",
    "        h = h + 2*delta\n",
    "\n",
    "        if x < 0 or y < 0 or x+w > image.shape[1] or y+h > image.shape[0]:\n",
    "            continue\n",
    "        # end                   \n",
    "\n",
    "        crop_img = image[y:y+h, x:x+w]   \n",
    "        # crop_img_arr.append(crop_img)\n",
    "        crop_img_arr.append(cv2.cvtColor(crop_img, cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "        if b_plot:    \n",
    "            plt.subplot(len_boxes, 1, index+1)\n",
    "            plt.imshow(crop_img)\n",
    "        # end\n",
    "\n",
    "    # end\n",
    "\n",
    "    if b_plot:\n",
    "        plt.show()\n",
    "    # end   \n",
    "    \n",
    "    if len_boxes == 0:\n",
    "        return {}\n",
    "    # end\n",
    "\n",
    "    image_tensors = preprocess_and_convert_to_tensorsarr(crop_img_arr, opt['imgW'], opt['imgH'])\n",
    "\n",
    "    pred_text_arr = run_ocr(ocr_model, ocr_converter, image_tensors, opt)\n",
    "    \n",
    "    if len(pred_text_arr) != len_boxes:\n",
    "        print('boxes len<', len_boxes, '> pred len<', len(pred_text_arr), '>')\n",
    "        # raise('pred and boxes dont match. Not expected to come here')\n",
    "        return []\n",
    "    # end\n",
    "    \n",
    "    return pred_text_arr    \n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_threshold = 0.7\n",
    "low_text = 0.4\n",
    "link_threshold = 0.4\n",
    "canvas_size = 1280\n",
    "mag_ratio = 1.5\n",
    "cuda = True\n",
    "show_time = False\n",
    "no_of_matches_required_for_team_names = 20\n",
    "skip_delta_in_secs = 10\n",
    "i_print_frequency = 10\n",
    "max_pos_error = 2\n",
    "\n",
    "opt = DotMap({\n",
    "    'imgW': 100,\n",
    "    'imgH': 32,\n",
    "    'workers' : 4,\n",
    "    'batch_size' : 192,\n",
    "    'saved_model' : 'dmodel/TPS-ResNet-BiLSTM-Attn.pth',\n",
    "    'batch_max_length' : 25  ,\n",
    "    'rgb' : False,\n",
    "    'character' :'0123456789abcdefghijklmnopqrstuvwxyz',\n",
    "    'sensitive' : False ,\n",
    "    'PAD' : False,\n",
    "    'Transformation' : 'TPS',\n",
    "    'FeatureExtraction' : 'ResNet',\n",
    "    'SequenceModeling' : 'BiLSTM',\n",
    "    'Prediction' : 'Attn',\n",
    "    'num_fiducial' : 20,\n",
    "    'input_channel' : 1,\n",
    "    'output_channel' : 512,\n",
    "    'hidden_size' : 256,\n",
    "    'num_gpu' : torch.cuda.device_count()\n",
    "})\n",
    "\n",
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = init_craft()\n",
    "ocr_model, ocr_converter = init_ocr(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = imgproc.loadImage('/mnt/disks/d1/s/code/sih/score_extraction/tempcandelete_15-20190407-game12/frame_213011.jpg')\n",
    "process_image(image, 'WAIN', 'ADR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_fullpath = '/mnt/disks/d1/s/data/apra/source/15-20190407-game12_source.mp4'\n",
    "t1_name = 'WAIN'\n",
    "t2_name = 'ADR'\n",
    "\n",
    "# net = init_craft()\n",
    "# ocr_model, ocr_converter = init_ocr(opt)\n",
    "\n",
    "v=cv2.VideoCapture(video_fullpath)\n",
    "\n",
    "i_counter = 0\n",
    "i_offset_in_secs = 350 # offset in seconds \n",
    "i_match_counter = 0\n",
    "\n",
    "pos_sec_arr = []\n",
    "pos_arr = []\n",
    "\n",
    "t1_min_box = [[10000 ,  10000],\n",
    "       [10000 ,  10000],\n",
    "       [10000  ,  10000 ],\n",
    "       [10000 ,  10000 ]]\n",
    "\n",
    "t2_min_box = [[10000 ,  10000],\n",
    "       [10000 ,  10000],\n",
    "       [10000  ,  10000 ],\n",
    "       [10000 ,  10000 ]]\n",
    "\n",
    "t1_max_box = [[0 ,  0],\n",
    "       [0 ,  0],\n",
    "       [0  ,  0 ],\n",
    "       [0 ,  0 ]]\n",
    "\n",
    "t2_max_box = [[0 ,  0],\n",
    "       [0 ,  0],\n",
    "       [0  ,  0 ],\n",
    "       [0 ,  0 ]]\n",
    "\n",
    "\n",
    "while True:\n",
    "    i_counter += 1\n",
    "    v.set(cv2.CAP_PROP_POS_MSEC, i_offset_in_secs*1000)\n",
    "    ret, frame = v.read()    \n",
    "    if ret != True:\n",
    "        break\n",
    "    # end\n",
    "    \n",
    "    img_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    matched_out = process_image(frame, t1_name, t2_name)\n",
    "    \n",
    "    if 't1' in matched_out and 't2' in matched_out:\n",
    "        i_match_counter += 1\n",
    "        pos_arr.append(matched_out)\n",
    "        pos_sec_arr.append(i_offset_in_secs)\n",
    "        \n",
    "        for pt_index, px_pnt in enumerate(matched_out['t1']):\n",
    "            for xy_index, val in enumerate(px_pnt):\n",
    "                if t1_min_box[pt_index][xy_index] > val:\n",
    "                    t1_min_box[pt_index][xy_index] = val\n",
    "                # end\n",
    "                if t1_max_box[pt_index][xy_index] < val:\n",
    "                    t1_max_box[pt_index][xy_index] = val\n",
    "                # end\n",
    "            # end            \n",
    "        # end\n",
    "        \n",
    "        for pt_index, px_pnt in enumerate(matched_out['t2']):\n",
    "            for xy_index, val in enumerate(px_pnt):\n",
    "                if t2_min_box[pt_index][xy_index] > val:\n",
    "                    t2_min_box[pt_index][xy_index] = val\n",
    "                # end\n",
    "                if t2_max_box[pt_index][xy_index] < val:\n",
    "                    t2_max_box[pt_index][xy_index] = val\n",
    "                # end\n",
    "            # end            \n",
    "        # end\n",
    "        \n",
    "    # end\n",
    "    \n",
    "    if i_match_counter == no_of_matches_required_for_team_names:\n",
    "        break\n",
    "    # end        \n",
    "    \n",
    "    if i_counter % i_print_frequency == 0:\n",
    "        print('processed<', i_counter, '> offset<', i_offset_in_secs, '> matched<', i_match_counter, '>')\n",
    "    # end\n",
    "    \n",
    "    i_offset_in_secs += skip_delta_in_secs    \n",
    "# end\n",
    "\n",
    "print('Completed. Processed<', i_counter, '> offset<', i_offset_in_secs, '>')\n",
    "\n",
    "i_deviated_error_counter = 0\n",
    "\n",
    "for pt_index, px_pnt in enumerate(t2_min_box):\n",
    "    for xy_index, val in enumerate(px_pnt):\n",
    "        delta_pos = t2_max_box[pt_index][xy_index] - val\n",
    "        if delta_pos > max_pos_error:\n",
    "            i_deviated_error_counter += 1\n",
    "        # end\n",
    "    # end            \n",
    "# end\n",
    "\n",
    "for pt_index, px_pnt in enumerate(t1_min_box):\n",
    "    for xy_index, val in enumerate(px_pnt):\n",
    "        delta_pos = t1_max_box[pt_index][xy_index] - val\n",
    "        if delta_pos > max_pos_error:\n",
    "            i_deviated_error_counter += 1\n",
    "        # end\n",
    "    # end            \n",
    "# end\n",
    "\n",
    "print('Deviation Counter<', i_deviated_error_counter, '>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_arr[0]['t1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_arr[1]['t1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_arr[11]['t1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sec_arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RepresentsIntTemp(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        if s == 'o':\n",
    "            return True\n",
    "        # end\n",
    "        return False\n",
    "    # end\n",
    "# end\n",
    "\n",
    "video_fullpath = '/mnt/disks/d1/s/data/apra/source/15-20190407-game12_source.mp4'   \n",
    "t1_name = 'wain'\n",
    "t2_name = 'adr'\n",
    "\n",
    "# net = init_craft()\n",
    "# ocr_model, ocr_converter = init_ocr(opt)\n",
    "\n",
    "v=cv2.VideoCapture(video_fullpath)\n",
    "\n",
    "i_counter = 0\n",
    "i_offset_in_secs = 480 # offset in seconds \n",
    "i_match_counter = 0\n",
    "\n",
    "missed_secs_arr = []\n",
    "\n",
    "b1 = np.array([[492, 32], [532, 32], [532, 72], [492, 72]]) # 492 32 40 40\n",
    "b2 = np.array([[746, 32], [786, 32], [786, 72], [746, 72]]) # 746 32 40 40\n",
    "\n",
    "\n",
    "while True:\n",
    "    i_counter += 1\n",
    "    v.set(cv2.CAP_PROP_POS_MSEC, i_offset_in_secs*1000)\n",
    "    ret, frame = v.read()    \n",
    "    if ret != True:\n",
    "        break\n",
    "    # end\n",
    "    \n",
    "    img_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # pred_text_arr = detect_scoreboard(frame, [pos_arr[0]['t1'], pos_arr[0]['t2']])\n",
    "    pred_text_arr = detect_scoreboard(frame, [b1, b2])\n",
    "    # print(pred_text_arr)\n",
    " \n",
    "    b_matched = True\n",
    "    if len(pred_text_arr) != 2 or RepresentsIntTemp(pred_text_arr[0]) == False or RepresentsIntTemp(pred_text_arr[1]) == False:\n",
    "        b_matched = False\n",
    "    # end\n",
    "                   \n",
    "    if b_matched:\n",
    "        i_match_counter += 1\n",
    "    else:\n",
    "        missed_secs_arr.append(i_offset_in_secs)        \n",
    "        cv2.imwrite('tempcandelete/frame_'+str(i_offset_in_secs).zfill(10)+'.jpg', frame)\n",
    "    # end\n",
    "    \n",
    "    if i_counter % i_print_frequency == 0:\n",
    "        print('processed<', i_counter, '> offset<', i_offset_in_secs, '> matched<', i_match_counter, '> missed<', len(missed_secs_arr), '>')\n",
    "    # end\n",
    "    \n",
    "    if i_counter > 1000000:\n",
    "        break\n",
    "    # end\n",
    "    \n",
    "    i_offset_in_secs += 10    \n",
    "# end\n",
    "\n",
    "print('Completed. Processed<', i_counter, '> offset<', i_offset_in_secs, '> matched<', i_match_counter, '> missed<', len(missed_secs_arr), '>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_secs_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_fullpath = '/mnt/disks/d1/s/data/apra/source/15-20190407-game12_source.mp4'   \n",
    "i_offset_in_secs = 1686 \n",
    "v=cv2.VideoCapture(video_fullpath)\n",
    "v.set(cv2.CAP_PROP_POS_MSEC, i_offset_in_secs*1000)\n",
    "ret, frame = v.read() \n",
    "cv2.imwrite('tempcandelete/frame_'+str(i_offset_in_secs).zfill(10)+'.jpg', frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RepresentsInt('2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
